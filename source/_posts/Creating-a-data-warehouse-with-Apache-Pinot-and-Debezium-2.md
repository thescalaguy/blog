---
title: Creating a data warehouse with Apache Pinot and Debezium 2
tags:
  - architecture
date: 2024-12-19 10:31:25
---

In the [previous post](/2024/12/18/Creating-a-data-wrehouse-with-Apache-Pinot-and-Debezium/) we saw how we can stream data from Debezium into Pinot. In the field-level transformations that we applied, we stored the primary key along with the entire "after" JSON generated by Debezium. In this post we'll look at how to extract fields from the "after" payload so that the data looks more tabular.  

## Getting Started  

The JSON stored in the `source` column contains the user agent of the device used to to place the order. Let's say we'd like to find out the most common user agents. To do this, we'd need to extract the value from the JSON and store it in a new column. This will be a two-step process. First, we'll update the schema to add a new "user_agent" column. Second, we'll update the table definition to reflect this change in schema.  

Let's start by updating the schema. We'll add the following column to the `dimensionFieldSpecs`.  

{% code %}
{
    "name": "user_agent",
    "dataType": "STRING",
    "notNull": false
}
{% endcode %}  

Notice how we've set `notNull` to `false`. This allows the column to contain null values in case the user agent is unavailable. Let's POST this updated schema to Pinot by executing the following curl command.  

{% code %}
curl -F schemaName=@tables/001-order/order_schema.json localhost:9000/schemas | jq .
{% endcode %}  

If we open the query console after updating the schema, we'll see the following error on the screen. It mentions that the segments are invalid because they were created from an older version of the schema and that to reflect the changes, we'd need to reload the table's segments. We'll get to this in a minute.

{% asset_img error.png %}  

Next, let's update the table by adding a new column. We'll add the following to the `transformConfigs`. Notice how we're referencing the `source` column in the `jsonPath` transform function.

{% code lang:json %}
{
    "columnName": "user_agent",
    "transformFunction": "jsonPath(source, '$.user_agent')"
}
{% endcode %}  

If we were to POST this new table definition, we'd get an error saying that the table already exists. We'll need to delete the table and recreate it. We delete the table and its associated metadata with the following curl command.  

{% code %}
curl -XDELETE localhost:9000/tables/orders | jq .
{% endcode %}  

We'll now POST the updated config for the `orders` table.  

{% code %}
curl -XPOST -H 'Content-Type: application/json' -d @tables/001-order/order_table.json localhost:9000/tables | jq .
{% endcode %}  

To ensure that the column is extracted even as more rows are generated by Debezium, we'll rerun the Python script which added dummy data.  

{% code lang:python %}
python faker/data.py
{% endcode %}  

We can now query the table and sort it in descending order on the `id` column to see the new rows. Upon doing so, we find that the column has been extracted properly. We can repeat the query and sort in ascending order to check if the same is true for older rows. 

{% asset_img query.png %}  

Let's repeat the exercise we did above but this time we'll add a new `dummy` column in the `orders` table in Postgres; its value will also be `dummy`. We'll execute the following SQL commands to add this new column.  

{% code lang:sql %}
ALTER TABLE public.orders ADD COLUMN dummy TEXT;
UPDATE public.orders SET dummy = 'dummy';
{% endcode %}  

This will cause Debezium to emit events into Kafka for every row and each of these rows will contain the new column with its values. Let's query Pinot to see this change reflected in the `source` column. We'll execute the following query.  

{% code lang:sql %}
select id, 
       JSON_EXTRACT_SCALAR(source, '$.dummy', 'STRING', '...') AS dummy
from orders limit 10
{% endcode %}  

Upon querying Pinot we find that `source` column in Pinot now contains the `dummy` field. We can see this in the screenshot below.  

{% asset_img dummy.png %}  

Although the example is contrived, it goes on to show that we can use Debezium to stream the latest state of the row into Pinot and use SQL to access this field. We could have, like we did for the `user_agent` column, extracted this into a column of its own, too. This avoids having to run expensive backfills since all we need to do is extract values from the `source` column in the table.  

Before we move on to the error that we saw earlier, we'll quickly discuss schema evolution in Pinot. In Pinot, we may only add columns to the table. If we'd like to drop a column or rename it, we'll have to create a brand new table.   

Now about the error that we saw earlier. Whenever a table's schema is updated or when indexes are modified, we need to reload all the segments. This can be done either from the UI or from the API. However, deleting and recreating a table performs a table re-bootstrap and applies the new schema and transform functions to it.   

That's it. That's how we can extract fields from the Debezium payload and evolve the schema.